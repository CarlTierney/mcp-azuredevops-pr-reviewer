"""
Contribution Analysis Module - Enhanced timing and quality metrics
"""

import os
import pandas as pd
import statistics
import sys
from datetime import datetime
from collections import defaultdict

# Ensure we can import from parent directories
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

class ContributionAnalyzer:
    def __init__(self, base_analyzer):
        self.analyzer = base_analyzer

    def analyze_commit_timing(self):
        """Analyze comprehensive contribution metrics with timing patterns and quality data"""
        print("\n=== COMPREHENSIVE CONTRIBUTION METRICS ANALYSIS ===")
        
        # Sort commits by date and calculate real timing metrics
        commits_with_dates = []
        for commit in self.analyzer.commits:
            author_info = self.analyzer.get_author_info(commit.get('author', {}))
            author_key = author_info['unique_name']
            
            try:
                commit_date_str = commit.get('author', {}).get('date', '')
                commit_date = datetime.fromisoformat(commit_date_str.replace('Z', '+00:00')) if commit_date_str else None
                
                if commit_date:
                    commits_with_dates.append({
                        'commit_id': commit.get('commitId', ''),
                        'author': author_key,
                        'date': commit_date,
                        'comment': commit.get('comment', ''),
                        'changes': commit.get('changeCounts', {})
                    })
            except (ValueError, AttributeError):
                continue
        
        # Sort by date
        commits_with_dates.sort(key=lambda x: x['date'])
        
        # Calculate comprehensive metrics by author
        author_metrics = defaultdict(lambda: {
            'timing_data': [],
            'commit_details': [],
            'commit_days': set(),
            'first_commit': None,
            'last_commit': None,
            'files_changed': set(),
            'loc_added': 0,
            'loc_deleted': 0,
            'loc_modified': 0,
            'non_whitespace_added': 0,
            'non_whitespace_deleted': 0,
            'non_whitespace_modified': 0,
            'complexity_samples': [],
            'file_types': defaultdict(int),
            'change_types': defaultdict(int)
        })
        
        # Process commits for timing analysis
        for i, commit in enumerate(commits_with_dates):
            author = commit['author']
            commit_date = commit['date']
            
            # Track all commit details for each author
            author_metrics[author]['commit_details'].append(commit_date)
            
            # Track first and last commit date by author
            if author_metrics[author]['first_commit'] is None:
                author_metrics[author]['first_commit'] = commit_date
            author_metrics[author]['last_commit'] = commit_date
            
            # Track unique days with commits
            commit_day = commit_date.date()
            author_metrics[author]['commit_days'].add(commit_day)
            
            # Find previous commit by same author for timing analysis
            prev_date = None
            for j in range(i-1, -1, -1):
                if commits_with_dates[j]['author'] == author:
                    prev_date = commits_with_dates[j]['date']
                    break
                    
            if prev_date:
                time_diff = (commit_date - prev_date).total_seconds() / 3600  # Hours
                author_metrics[author]['timing_data'].append(time_diff)
        
        # Get repository ID for enhanced quality analysis
        try:
            repo_id = self.analyzer.get_repository_id()
        except Exception as e:
            print(f"Warning: Could not get repository ID: {e}")
            repo_id = None
        
        # Analyze detailed commits for quality metrics
        print("Integrating quality metrics from detailed commits...")
        for commit_id, changes_data in self.analyzer.detailed_commits.items():
            commit_info = next((c for c in self.analyzer.commits if c.get('commitId') == commit_id), None)
            if not commit_info:
                continue
                
            author_info = self.analyzer.get_author_info(commit_info.get('author', {}))
            author_key = author_info['unique_name']
            
            changes = changes_data.get('changes', [])
            for change in changes:
                item = change.get('item', {})
                change_type = change.get('changeType', '')
                path = item.get('path', '')
                
                if not path or item.get('isFolder', False):
                    continue
                
                filename = os.path.basename(path)
                file_type = self.analyzer.classify_file_type(filename)
                
                # Skip non-code files
                if file_type in ['other', 'docs']:
                    continue
                
                author_metrics[author_key]['files_changed'].add(path)
                author_metrics[author_key]['file_types'][file_type] += 1
                author_metrics[author_key]['change_types'][change_type] += 1
                
                # Analyze file content for quality metrics
                if repo_id:
                    content = self.analyzer.fetch_file_content(repo_id, commit_id, path)
                    if content:
                        file_analysis = self.analyzer.analyze_file_contents(content)
                        complexity = self.analyzer.calculate_cyclomatic_complexity(content, filename)
                        
                        if complexity > 1:
                            author_metrics[author_key]['complexity_samples'].append(complexity)
                        
                        # Calculate real LOC metrics based on change type
                        if change_type == 'add':
                            author_metrics[author_key]['loc_added'] += file_analysis['loc']
                            author_metrics[author_key]['non_whitespace_added'] += file_analysis['sloc']
                        elif change_type == 'edit':
                            # Estimate 30% of file was modified
                            loc_modified = int(file_analysis['loc'] * 0.3)
                            non_ws_modified = int(file_analysis['sloc'] * 0.3)
                            author_metrics[author_key]['loc_modified'] += loc_modified
                            author_metrics[author_key]['non_whitespace_modified'] += non_ws_modified
                        elif change_type == 'delete':
                            author_metrics[author_key]['loc_deleted'] += file_analysis['loc']
                            author_metrics[author_key]['non_whitespace_deleted'] += file_analysis['sloc']
                else:
                    # Fallback estimates
                    if change_type == 'add':
                        author_metrics[author_key]['loc_added'] += 50
                        author_metrics[author_key]['non_whitespace_added'] += 40
                    elif change_type == 'edit':
                        author_metrics[author_key]['loc_modified'] += 15
                        author_metrics[author_key]['non_whitespace_modified'] += 12
                    elif change_type == 'delete':
                        author_metrics[author_key]['loc_deleted'] += 25
                        author_metrics[author_key]['non_whitespace_deleted'] += 20
        
        # Calculate comprehensive contribution metrics
        contribution_data = []
        
        for author_key, metrics in author_metrics.items():
            commit_dates = metrics['commit_details']
            time_diffs = metrics['timing_data']
            
            if not commit_dates:
                continue
            
            total_commits = len(commit_dates)
            active_days = len(metrics['commit_days'])
            first_commit = metrics['first_commit']
            last_commit = metrics['last_commit']
            
            # Calculate timing metrics
            active_period_days = max(1, (last_commit - first_commit).days + 1)
            commits_per_day = total_commits / active_period_days
            commits_per_active_day = total_commits / max(1, active_days)
            
            # Calculate time between commits
            avg_hours_between_commits = sum(time_diffs) / max(1, len(time_diffs)) if time_diffs else 0
            median_hours_between_commits = statistics.median(time_diffs) if time_diffs else 0
            
            # Calculate work pattern percentages with real data
            business_hours_commits = 0
            weekend_commits = 0
            night_commits = 0
            
            for commit_date in commit_dates:
                is_weekend = commit_date.weekday() >= 5  # 5=Saturday, 6=Sunday
                is_business_hours = 9 <= commit_date.hour < 17
                is_night = commit_date.hour < 6 or commit_date.hour >= 22
                
                if is_weekend:
                    weekend_commits += 1
                if is_business_hours and not is_weekend:
                    business_hours_commits += 1
                if is_night:
                    night_commits += 1
            
            business_hours_pct = (business_hours_commits / total_commits) * 100
            weekend_pct = (weekend_commits / total_commits) * 100
            night_pct = (night_commits / total_commits) * 100
            
            # Calculate quality metrics
            total_files_changed = len(metrics['files_changed'])
            total_loc_added = metrics['loc_added']
            total_loc_deleted = metrics['loc_deleted']
            total_loc_modified = metrics['loc_modified']
            total_loc_net = total_loc_added - total_loc_deleted
            
            total_non_ws_added = metrics['non_whitespace_added']
            total_non_ws_deleted = metrics['non_whitespace_deleted']
            total_non_ws_modified = metrics['non_whitespace_modified']
            total_non_ws_net = total_non_ws_added - total_non_ws_deleted
            
            # Calculate ratios
            total_loc_changes = total_loc_added + total_loc_modified
            total_non_ws_changes = total_non_ws_added + total_non_ws_modified
            non_whitespace_ratio = (total_non_ws_changes / max(total_loc_changes, 1)) if total_loc_changes > 0 else 0
            
            # Calculate complexity metrics
            avg_complexity = (sum(metrics['complexity_samples']) / len(metrics['complexity_samples'])) if metrics['complexity_samples'] else 0
            high_complexity_count = sum(1 for c in metrics['complexity_samples'] if c > 10)
            
            # Calculate productivity metrics
            avg_loc_per_commit = total_loc_changes / max(total_commits, 1)
            avg_files_per_commit = total_files_changed / max(total_commits, 1)
            
            # Determine primary file type
            primary_file_type = max(metrics['file_types'].items(), key=lambda x: x[1])[0] if metrics['file_types'] else 'unknown'
            
            # Calculate overall contribution score
            productivity_score = min(100, (total_commits * 0.3) + (total_files_changed * 0.2) + (total_loc_net / 100 * 0.5))
            quality_score = min(100, (non_whitespace_ratio * 30) + ((10 - min(avg_complexity, 10)) * 7))
            consistency_score = min(100, (commits_per_active_day * 20) + ((1 / max(avg_hours_between_commits / 24, 0.1)) * 10))
            
            overall_contribution_score = (productivity_score + quality_score + consistency_score) / 3
            
            contribution_data.append({
                'Developer': author_key,
                'Total_Commits': total_commits,
                'Active_Days': active_days,
                'Active_Period_Days': active_period_days,
                'Commits_Per_Day': round(commits_per_day, 2),
                'Commits_Per_Active_Day': round(commits_per_active_day, 2),
                'Avg_Hours_Between_Commits': round(avg_hours_between_commits, 2),
                'Median_Hours_Between_Commits': round(median_hours_between_commits, 2),
                'Business_Hours_Commits_Pct': round(business_hours_pct, 1),
                'Weekend_Commits_Pct': round(weekend_pct, 1),
                'Night_Commits_Pct': round(night_pct, 1),
                'Total_Files_Changed': total_files_changed,
                'LOC_Added': total_loc_added,
                'LOC_Deleted': total_loc_deleted,
                'LOC_Modified': total_loc_modified,
                'LOC_Net_Change': total_loc_net,
                'Non_Whitespace_Added': total_non_ws_added,
                'Non_Whitespace_Deleted': total_non_ws_deleted,
                'Non_Whitespace_Modified': total_non_ws_modified,
                'Non_Whitespace_Net': total_non_ws_net,
                'Non_Whitespace_Ratio': round(non_whitespace_ratio, 3),
                'Avg_Cyclomatic_Complexity': round(avg_complexity, 2),
                'High_Complexity_Files': high_complexity_count,
                'Avg_LOC_Per_Commit': round(avg_loc_per_commit, 1),
                'Avg_Files_Per_Commit': round(avg_files_per_commit, 1),
                'Primary_File_Type': primary_file_type,
                'Add_Changes': metrics['change_types'].get('add', 0),
                'Edit_Changes': metrics['change_types'].get('edit', 0),
                'Delete_Changes': metrics['change_types'].get('delete', 0),
                'Productivity_Score': round(productivity_score, 1),
                'Quality_Score': round(quality_score, 1),
                'Consistency_Score': round(consistency_score, 1),
                'Overall_Contribution_Score': round(overall_contribution_score, 1),
                'First_Commit': first_commit.strftime('%Y-%m-%d') if first_commit else 'Unknown',
                'Last_Commit': last_commit.strftime('%Y-%m-%d') if last_commit else 'Unknown'
            })
        
        df_contributions = pd.DataFrame(contribution_data)
        if not df_contributions.empty:
            df_contributions = df_contributions.sort_values('Overall_Contribution_Score', ascending=False)
            
            print(f"‚úì Analyzed comprehensive contribution metrics for {len(df_contributions)} developers")
            
            # Display key metrics
            summary_cols = ['Developer', 'Total_Commits', 'LOC_Net_Change', 'Non_Whitespace_Ratio', 
                           'Business_Hours_Commits_Pct', 'Overall_Contribution_Score']
            print("\nContribution Metrics Summary:")
            print(df_contributions[summary_cols].to_string(index=False))
            
            # Save as contribution metrics
            df_contributions.to_csv(f"{self.analyzer.data_dir}/azdo_contribution_metrics.csv", index=False)
            
            # Print comprehensive insights
            print(f"\n=== CONTRIBUTION INSIGHTS ===")
            avg_business_hours = df_contributions['Business_Hours_Commits_Pct'].mean()
            avg_weekend = df_contributions['Weekend_Commits_Pct'].mean()
            avg_quality = df_contributions['Quality_Score'].mean()
            
            print(f"üìä Team Averages:")
            print(f"  ‚Ä¢ Business hours commits: {avg_business_hours:.1f}%")
            print(f"  ‚Ä¢ Weekend commits: {avg_weekend:.1f}%")
            print(f"  ‚Ä¢ Code quality score: {avg_quality:.1f}/100")
            print(f"  ‚Ä¢ Average contribution score: {df_contributions['Overall_Contribution_Score'].mean():.1f}/100")
            
            top_contributor = df_contributions.iloc[0]
            print(f"\nüèÜ Top Contributor: {top_contributor['Developer']}")
            print(f"  ‚Ä¢ {top_contributor['Total_Commits']} commits, {top_contributor['LOC_Net_Change']:,} net LOC")
            print(f"  ‚Ä¢ Quality score: {top_contributor['Quality_Score']:.1f}, Consistency: {top_contributor['Consistency_Score']:.1f}")
            
            # Identify work patterns
            night_owls = df_contributions[df_contributions['Night_Commits_Pct'] > 20]
            weekend_warriors = df_contributions[df_contributions['Weekend_Commits_Pct'] > 30]
            
            if not night_owls.empty:
                print(f"\nüåô Night Owls (>20% night commits): {', '.join(night_owls['Developer'].tolist())}")
            if not weekend_warriors.empty:
                print(f"üìÖ Weekend Warriors (>30% weekend commits): {', '.join(weekend_warriors['Developer'].tolist())}")
        
        return df_contributions

    def analyze_advanced_developer_contributions(self):
        """Analyze advanced developer contributions with real metrics"""
        print("\n=== ADVANCED DEVELOPER CONTRIBUTIONS ANALYSIS ===")
        
        # Build comprehensive developer metrics
        developer_metrics = defaultdict(lambda: {
            'commits': 0,
            'files_changed': set(),
            'loc_added': 0,
            'loc_modified': 0,
            'complexity_samples': [],
            'collaborators': set(),
            'pull_requests': 0,
            'code_reviews': 0,
            'knowledge_areas': set(),
            'file_types': defaultdict(int),
            'commit_messages': []
        })
        
        # Analyze commits for productivity and quality
        for commit_id, changes_data in self.analyzer.detailed_commits.items():
            commit_info = next((c for c in self.analyzer.commits if c.get('commitId') == commit_id), None)
            if not commit_info:
                continue
            
            author_info = self.analyzer.get_author_info(commit_info.get('author', {}))
            author_key = author_info['unique_name']
            
            developer_metrics[author_key]['commits'] += 1
            developer_metrics[author_key]['commit_messages'].append(commit_info.get('comment', ''))
            
            # Analyze file changes
            changes = changes_data.get('changes', [])
            for change in changes:
                item = change.get('item', {})
                path = item.get('path', '')
                change_type = change.get('changeType', '')
                
                if not path or item.get('isFolder', False):
                    continue
                
                filename = os.path.basename(path)
                file_type = self.analyzer.classify_file_type(filename)
                
                if file_type not in ['other', 'docs']:
                    developer_metrics[author_key]['files_changed'].add(path)
                    developer_metrics[author_key]['file_types'][file_type] += 1
                    developer_metrics[author_key]['knowledge_areas'].add(file_type)
                    
                    # Get file content for quality analysis
                    try:
                        repo_id = self.analyzer.get_repository_id()
                        content = self.analyzer.fetch_file_content(repo_id, commit_id, path)
                        if content:
                            file_analysis = self.analyzer.analyze_file_contents(content)
                            complexity = self.analyzer.calculate_cyclomatic_complexity(content, filename)
                            
                            if complexity > 1:
                                developer_metrics[author_key]['complexity_samples'].append(complexity)
                            
                            if change_type == 'add':
                                developer_metrics[author_key]['loc_added'] += file_analysis['sloc']
                            elif change_type == 'edit':
                                developer_metrics[author_key]['loc_modified'] += int(file_analysis['sloc'] * 0.3)
                    except:
                        # Fallback estimates
                        if change_type == 'add':
                            developer_metrics[author_key]['loc_added'] += 40
                        elif change_type == 'edit':
                            developer_metrics[author_key]['loc_modified'] += 12
        
        # Analyze pull requests for collaboration
        for pr in self.analyzer.pull_requests:
            creator_info = self.analyzer.get_author_info(pr.get('createdBy', {}))
            creator_key = creator_info['unique_name']
            
            developer_metrics[creator_key]['pull_requests'] += 1
            
            # Track reviewers as collaborators
            reviewers = pr.get('reviewers', [])
            for reviewer in reviewers:
                reviewer_info = self.analyzer.get_author_info(reviewer)
                reviewer_key = reviewer_info['unique_name']
                
                if reviewer_key != creator_key:
                    developer_metrics[creator_key]['collaborators'].add(reviewer_key)
                    developer_metrics[reviewer_key]['code_reviews'] += 1
                    developer_metrics[reviewer_key]['collaborators'].add(creator_key)
        
        # Calculate advanced metrics
        advanced_data = []
        
        for developer, metrics in developer_metrics.items():
            if metrics['commits'] == 0:
                continue
            
            # Productivity Score (0-100)
            commit_score = min(metrics['commits'] / 10 * 25, 25)  # Up to 25 points for commits
            file_score = min(len(metrics['files_changed']) / 20 * 25, 25)  # Up to 25 points for files
            loc_score = min((metrics['loc_added'] + metrics['loc_modified']) / 1000 * 25, 25)  # Up to 25 points for LOC
            pr_score = min(metrics['pull_requests'] / 5 * 25, 25)  # Up to 25 points for PRs
            productivity_score = commit_score + file_score + loc_score + pr_score
            
            # Quality Score (0-100)
            avg_complexity = sum(metrics['complexity_samples']) / len(metrics['complexity_samples']) if metrics['complexity_samples'] else 5
            complexity_score = max(0, (10 - min(avg_complexity, 10)) * 10)  # Lower complexity = higher score
            
            # Analyze commit message quality
            commit_messages = metrics['commit_messages']
            detailed_messages = sum(1 for msg in commit_messages if len(msg.split()) >= 3)
            message_quality = (detailed_messages / max(len(commit_messages), 1)) * 40
            
            # Code review participation
            review_score = min(metrics['code_reviews'] / 10 * 20, 20)
            
            quality_score = complexity_score + message_quality + review_score
            
            # Knowledge Sharing Score (0-100)
            knowledge_breadth = min(len(metrics['knowledge_areas']) / 5 * 30, 30)  # Up to 30 for breadth
            collaboration_score = min(len(metrics['collaborators']) / 5 * 40, 40)  # Up to 40 for collaboration
            pr_sharing = min(metrics['pull_requests'] / 10 * 30, 30)  # Up to 30 for PR creation
            knowledge_sharing_score = knowledge_breadth + collaboration_score + pr_sharing
            
            # Specialization analysis
            primary_language = max(metrics['file_types'].items(), key=lambda x: x[1])[0] if metrics['file_types'] else 'unknown'
            
            advanced_data.append({
                'Developer': developer,
                'Commits': metrics['commits'],
                'Files_Changed': len(metrics['files_changed']),
                'LOC_Added': metrics['loc_added'],
                'LOC_Modified': metrics['loc_modified'],
                'Pull_Requests': metrics['pull_requests'],
                'Code_Reviews': metrics['code_reviews'],
                'Collaborators': len(metrics['collaborators']),
                'Collaborator_List': ', '.join(sorted(list(metrics['collaborators']))),
                'Knowledge_Areas': len(metrics['knowledge_areas']),
                'Knowledge_List': ', '.join(sorted(list(metrics['knowledge_areas']))),
                'Primary_Language': primary_language,
                'Avg_Complexity': round(avg_complexity, 2),
                'Detailed_Messages_Pct': round((detailed_messages / max(len(commit_messages), 1)) * 100, 1),
                'Productivity_Score': round(productivity_score, 1),
                'Quality_Score': round(quality_score, 1),
                'Knowledge_Sharing_Score': round(knowledge_sharing_score, 1),
                'Overall_Score': round((productivity_score + quality_score + knowledge_sharing_score) / 3, 1)
            })
        
        # Create DataFrame and save
        df_advanced = pd.DataFrame(advanced_data)
        if not df_advanced.empty:
            df_advanced = df_advanced.sort_values('Overall_Score', ascending=False)
            
            print(f"‚úÖ Analyzed advanced contributions for {len(df_advanced)} developers")
            
            # Display summary
            summary_cols = ['Developer', 'Productivity_Score', 'Quality_Score', 'Knowledge_Sharing_Score', 'Overall_Score']
            print("\nAdvanced Developer Contributions:")
            print(df_advanced[summary_cols].to_string(index=False))
            
            # Save results
            output_file = f"{self.analyzer.data_dir}/azdo_advanced_developer_contributions.csv"
            df_advanced.to_csv(output_file, index=False)
            print(f"üíæ Saved advanced contributions to: {output_file}")
            
            # Generate insights
            print(f"\n=== ADVANCED CONTRIBUTION INSIGHTS ===")
            top_overall = df_advanced.iloc[0]
            print(f"üèÜ Top Overall Contributor: {top_overall['Developer']}")
            print(f"  ‚Ä¢ Overall Score: {top_overall['Overall_Score']}/100")
            print(f"  ‚Ä¢ Productivity: {top_overall['Productivity_Score']}/100")
            print(f"  ‚Ä¢ Quality: {top_overall['Quality_Score']}/100")
            print(f"  ‚Ä¢ Knowledge Sharing: {top_overall['Knowledge_Sharing_Score']}/100")
            
            # Find specialists and generalists
            top_productivity = df_advanced.nlargest(1, 'Productivity_Score').iloc[0]
            top_quality = df_advanced.nlargest(1, 'Quality_Score').iloc[0]
            top_knowledge = df_advanced.nlargest(1, 'Knowledge_Sharing_Score').iloc[0]
            
            print(f"\nüéØ Category Leaders:")
            print(f"  ‚Ä¢ Most Productive: {top_productivity['Developer']} ({top_productivity['Productivity_Score']}/100)")
            print(f"  ‚Ä¢ Highest Quality: {top_quality['Developer']} ({top_quality['Quality_Score']}/100)")
            print(f"  ‚Ä¢ Best Knowledge Sharing: {top_knowledge['Developer']} ({top_knowledge['Knowledge_Sharing_Score']}/100)")
            
            # Identify collaboration patterns
            high_collaborators = df_advanced[df_advanced['Collaborators'] >= 3]
            if not high_collaborators.empty:
                print(f"\nü§ù Strong Collaborators (3+ team members):")
                for _, dev in high_collaborators.iterrows():
                    print(f"  ‚Ä¢ {dev['Developer']}: {dev['Collaborators']} collaborators")
        
        return df_advanced

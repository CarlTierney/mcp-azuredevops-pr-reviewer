"""
Contribution Analysis Module - Enhanced timing and quality metrics
"""

import os
import pandas as pd
import statistics
import sys
from datetime import datetime
from collections import defaultdict

# Ensure we can import from parent directories
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

class ContributionAnalyzer:
    def __init__(self, base_analyzer):
        self.analyzer = base_analyzer

    def analyze_commit_timing(self):
        """Analyze comprehensive contribution metrics with timing patterns and quality data"""
        print("\n=== COMPREHENSIVE CONTRIBUTION METRICS ANALYSIS ===")
        
        # Check if analysis is already cached
        analysis_name = "contribution_metrics"
        if self.analyzer.is_analysis_cached(analysis_name):
            cached_df = self.analyzer.get_cached_dataframe(analysis_name)
            if cached_df is not None:
                print("[STATS] Displaying cached contribution metrics results:")
                summary_cols = ['Developer', 'Total_Commits', 'LOC_Net_Change', 'Non_Whitespace_Ratio', 
                               'Business_Hours_Commits_Pct', 'Overall_Contribution_Score']
                print("\nContribution Metrics Summary:")
                print(cached_df[summary_cols].to_string(index=False))
                return cached_df
        
        print("[SEARCH] Running fresh contribution metrics analysis...")
        
        # Sort commits by date and calculate real timing metrics
        print("[STATS] Preparing commit data for timing analysis...")
        commits_with_dates = []
        for commit in self.analyzer.commits:
            author_info = self.analyzer.get_author_info(commit.get('author', {}))
            author_key = author_info['unique_name']
            
            try:
                commit_date_str = commit.get('author', {}).get('date', '')
                commit_date = datetime.fromisoformat(commit_date_str.replace('Z', '+00:00')) if commit_date_str else None
                
                if commit_date:
                    commits_with_dates.append({
                        'commit_id': commit.get('commitId', ''),
                        'author': author_key,
                        'date': commit_date,
                        'comment': commit.get('comment', ''),
                        'changes': commit.get('changeCounts', {})
                    })
            except (ValueError, AttributeError):
                continue
        
        print(f"  [OK] Prepared {len(commits_with_dates):,} commits for analysis")
        
        # Sort by date
        print("[DATE] Sorting commits by date...")
        commits_with_dates.sort(key=lambda x: x['date'])
        
        # Calculate comprehensive metrics by author
        print("[AUTHORS] Calculating timing metrics by author...")
        author_metrics = defaultdict(lambda: {
            'timing_data': [],
            'commit_details': [],
            'commit_days': set(),
            'first_commit': None,
            'last_commit': None,
            'files_changed': set(),
            'loc_added': 0,
            'loc_deleted': 0,
            'loc_modified': 0,
            'non_whitespace_added': 0,
            'non_whitespace_deleted': 0,
            'non_whitespace_modified': 0,
            'complexity_samples': [],
            'file_types': defaultdict(int),
            'change_types': defaultdict(int)
        })
        
        # Process commits for timing analysis
        print("[TIME]  Processing commits for timing patterns...")
        for i, commit in enumerate(commits_with_dates):
            if i % 1000 == 0:
                progress_pct = (i / len(commits_with_dates)) * 100
                print(f"    Timing analysis: {i:,}/{len(commits_with_dates):,} ({progress_pct:.1f}%)")
            
            author = commit['author']
            commit_date = commit['date']
            
            # Track all commit details for each author
            author_metrics[author]['commit_details'].append(commit_date)
            
            # Track first and last commit date by author
            if author_metrics[author]['first_commit'] is None:
                author_metrics[author]['first_commit'] = commit_date
            author_metrics[author]['last_commit'] = commit_date
            
            # Track unique days with commits
            commit_day = commit_date.date()
            author_metrics[author]['commit_days'].add(commit_day)
            
            # Find previous commit by same author for timing analysis
            prev_date = None
            for j in range(i-1, -1, -1):
                if commits_with_dates[j]['author'] == author:
                    prev_date = commits_with_dates[j]['date']
                    break
                    
            if prev_date:
                time_diff = (commit_date - prev_date).total_seconds() / 3600  # Hours
                author_metrics[author]['timing_data'].append(time_diff)
        
        print(f"  [OK] Completed timing analysis for {len(author_metrics)} developers")
        
        # Get repository ID for enhanced quality analysis
        try:
            repo_id = self.analyzer.get_repository_id()
            print(f"  [OK] Repository ID obtained for quality analysis: {repo_id}")
        except Exception as e:
            print(f"  [WARNING]  Warning: Could not get repository ID: {e}")
            repo_id = None
        
        # Analyze detailed commits for quality metrics with aggressive timeouts
        total_detailed = len(self.analyzer.detailed_commits)
        if total_detailed == 0:
            print("  [WARNING]  No detailed commits available for quality analysis")
        else:
            print(f"[SEARCH] Integrating quality metrics from {total_detailed:,} detailed commits...")
            
        processed_commits = 0
        processed_files = 0
        skipped_files = 0
        
        # Add time tracking for quality analysis
        import time
        quality_start_time = time.time()
        last_quality_progress = quality_start_time
        elapsed = 0  # Initialize elapsed variable
        
        for commit_id, changes_data in self.analyzer.detailed_commits.items():
            processed_commits += 1
            current_time = time.time()
            elapsed = current_time - quality_start_time  # Update elapsed time
            
            # More frequent progress reporting every 25 commits OR every 15 seconds
            if processed_commits % 25 == 0 or (current_time - last_quality_progress) > 15:
                progress_pct = (processed_commits / total_detailed) * 100
                commits_per_sec = processed_commits / max(elapsed, 1)
                eta_seconds = (total_detailed - processed_commits) / max(commits_per_sec, 0.1)
                
                print(f"    Quality analysis: {processed_commits:,}/{total_detailed:,} ({progress_pct:.1f}%)")
                print(f"      Files: {processed_files:,} processed | {skipped_files:,} skipped")
                print(f"      Speed: {commits_per_sec:.1f} commits/sec | ETA: {eta_seconds/60:.1f} minutes")
                last_quality_progress = current_time
            
            # Log current processing status for visibility
            if processed_commits % 25 == 0:
                current_commit_short = commit_id[:8] if commit_id else "unknown"
                print(f"    [LOCATION] Currently processing commit: {current_commit_short} ({processed_commits}/{total_detailed})")
            
            commit_info = next((c for c in self.analyzer.commits if c.get('commitId') == commit_id), None)
            if not commit_info:
                continue
                
            author_info = self.analyzer.get_author_info(commit_info.get('author', {}))
            author_key = author_info['unique_name']
            
            changes = changes_data.get('changes', [])
            
            # Process all changes without artificial limits for comprehensive analysis
            if len(changes) > 200:
                print(f"      Large commit detected ({len(changes)} changes), processing all changes...")
            
            for change in changes:
                item = change.get('item', {})
                change_type = change.get('changeType', '')
                path = item.get('path', '')
                
                if not path or item.get('isFolder', False):
                    continue
                
                filename = os.path.basename(path)
                file_type = self.analyzer.classify_file_type(filename)
                
                # Skip non-code files
                if file_type in ['other', 'docs']:
                    skipped_files += 1
                    continue
                
                author_metrics[author_key]['files_changed'].add(path)
                author_metrics[author_key]['file_types'][file_type] += 1
                author_metrics[author_key]['change_types'][change_type] += 1
                processed_files += 1
                
                # Analyze file content with aggressive timeout protection
                if repo_id and elapsed < 1200:  # Only fetch content for first 20 minutes
                    try:
                        # Quick timeout for individual files
                        file_start_time = time.time()
                        
                        content = self.analyzer.fetch_file_content(repo_id, commit_id, path)
                        
                        # Check if file fetch took too long
                        if time.time() - file_start_time > 5:  # 5 second timeout per file
                            skipped_files += 1
                            continue
                        
                        if content:
                            file_analysis = self.analyzer.analyze_file_contents(content)
                            complexity = self.analyzer.calculate_cyclomatic_complexity(content, filename)
                            
                            if complexity > 1:
                                author_metrics[author_key]['complexity_samples'].append(complexity)
                            
                            # Calculate real LOC metrics based on change type
                            if change_type == 'add':
                                author_metrics[author_key]['loc_added'] += file_analysis['loc']
                                author_metrics[author_key]['non_whitespace_added'] += file_analysis['sloc']
                            elif change_type == 'edit':
                                # Estimate 30% of file was modified
                                loc_modified = int(file_analysis['loc'] * 0.3)
                                non_ws_modified = int(file_analysis['sloc'] * 0.3)
                                author_metrics[author_key]['loc_modified'] += loc_modified
                                author_metrics[author_key]['non_whitespace_modified'] += non_ws_modified
                            elif change_type == 'delete':
                                author_metrics[author_key]['loc_deleted'] += file_analysis['loc']
                                author_metrics[author_key]['non_whitespace_deleted'] += file_analysis['sloc']
                        else:
                            skipped_files += 1
                            
                    except Exception as e:
                        # Only log errors occasionally to avoid spam
                        if processed_files % 500 == 0:
                            print(f"      Warning: Failed to analyze {filename}: {type(e).__name__}")
                        skipped_files += 1
                        
                        # Use fallback estimates
                        if change_type == 'add':
                            author_metrics[author_key]['loc_added'] += 50
                            author_metrics[author_key]['non_whitespace_added'] += 40
                        elif change_type == 'edit':
                            author_metrics[author_key]['loc_modified'] += 15
                            author_metrics[author_key]['non_whitespace_modified'] += 12
                        elif change_type == 'delete':
                            author_metrics[author_key]['loc_deleted'] += 25
                            author_metrics[author_key]['non_whitespace_deleted'] += 20
                else:
                    # Use fallback estimates when skipping content analysis
                    skipped_files += 1
                    if change_type == 'add':
                        author_metrics[author_key]['loc_added'] += 50
                        author_metrics[author_key]['non_whitespace_added'] += 40
                    elif change_type == 'edit':
                        author_metrics[author_key]['loc_modified'] += 15
                        author_metrics[author_key]['non_whitespace_modified'] += 12
                    elif change_type == 'delete':
                        author_metrics[author_key]['loc_deleted'] += 25
                        author_metrics[author_key]['non_whitespace_deleted'] += 20
            
            # Emergency brake for too many processed files
            if processed_files > 10000:
                print(f"    [WARNING]  Processed {processed_files:,} files, switching to estimates for remaining commits...")
                break
        
        quality_time = time.time() - quality_start_time
        print(f"  [OK] Completed quality analysis in {quality_time/60:.1f} minutes:")
        print(f"    - {processed_commits:,} commits processed")
        print(f"    - {processed_files:,} files analyzed")
        print(f"    - {skipped_files:,} files skipped")
        
        # Calculate comprehensive contribution metrics
        print("[CHART] Calculating comprehensive contribution scores...")
        contribution_data = []
        
        for author_key, metrics in author_metrics.items():
            commit_dates = metrics['commit_details']
            time_diffs = metrics['timing_data']
            
            if not commit_dates:
                continue
            
            total_commits = len(commit_dates)
            active_days = len(metrics['commit_days'])
            first_commit = metrics['first_commit']
            last_commit = metrics['last_commit']
            
            # Calculate timing metrics
            active_period_days = max(1, (last_commit - first_commit).days + 1)
            commits_per_day = total_commits / active_period_days
            commits_per_active_day = total_commits / max(1, active_days)
            
            # Calculate time between commits
            avg_hours_between_commits = sum(time_diffs) / max(1, len(time_diffs)) if time_diffs else 0
            median_hours_between_commits = statistics.median(time_diffs) if time_diffs else 0
            
            # Calculate work pattern percentages with real data
            business_hours_commits = 0
            weekend_commits = 0
            night_commits = 0
            
            for commit_date in commit_dates:
                is_weekend = commit_date.weekday() >= 5  # 5=Saturday, 6=Sunday
                is_business_hours = 9 <= commit_date.hour < 17
                is_night = commit_date.hour < 6 or commit_date.hour >= 22
                
                if is_weekend:
                    weekend_commits += 1
                if is_business_hours and not is_weekend:
                    business_hours_commits += 1
                if is_night:
                    night_commits += 1
            
            business_hours_pct = (business_hours_commits / total_commits) * 100
            weekend_pct = (weekend_commits / total_commits) * 100
            night_pct = (night_commits / total_commits) * 100
            
            # Calculate quality metrics
            total_files_changed = len(metrics['files_changed'])
            total_loc_added = metrics['loc_added']
            total_loc_deleted = metrics['loc_deleted']
            total_loc_modified = metrics['loc_modified']
            total_loc_net = total_loc_added - total_loc_deleted
            
            total_non_ws_added = metrics['non_whitespace_added']
            total_non_ws_deleted = metrics['non_whitespace_deleted']
            total_non_ws_modified = metrics['non_whitespace_modified']
            total_non_ws_net = total_non_ws_added - total_non_ws_deleted
            
            # Calculate ratios
            total_loc_changes = total_loc_added + total_loc_modified
            total_non_ws_changes = total_non_ws_added + total_non_ws_modified
            non_whitespace_ratio = (total_non_ws_changes / max(total_loc_changes, 1)) if total_loc_changes > 0 else 0
            
            # Calculate complexity metrics
            avg_complexity = (sum(metrics['complexity_samples']) / len(metrics['complexity_samples'])) if metrics['complexity_samples'] else 0
            high_complexity_count = sum(1 for c in metrics['complexity_samples'] if c > 10)
            
            # Calculate productivity metrics
            avg_loc_per_commit = total_loc_changes / max(total_commits, 1)
            avg_files_per_commit = total_files_changed / max(total_commits, 1)
            
            # Determine primary file type
            primary_file_type = max(metrics['file_types'].items(), key=lambda x: x[1])[0] if metrics['file_types'] else 'unknown'
            
            # Calculate overall contribution score
            productivity_score = min(100, (total_commits * 0.3) + (total_files_changed * 0.2) + (total_loc_net / 100 * 0.5))
            quality_score = min(100, (non_whitespace_ratio * 30) + ((10 - min(avg_complexity, 10)) * 7))
            consistency_score = min(100, (commits_per_active_day * 20) + ((1 / max(avg_hours_between_commits / 24, 0.1)) * 10))
            
            overall_contribution_score = (productivity_score + quality_score + consistency_score) / 3
            
            contribution_data.append({
                'Developer': author_key,
                'Total_Commits': total_commits,
                'Active_Days': active_days,
                'Active_Period_Days': active_period_days,
                'Commits_Per_Day': round(commits_per_day, 2),
                'Commits_Per_Active_Day': round(commits_per_active_day, 2),
                'Avg_Hours_Between_Commits': round(avg_hours_between_commits, 2),
                'Median_Hours_Between_Commits': round(median_hours_between_commits, 2),
                'Business_Hours_Commits_Pct': round(business_hours_pct, 1),
                'Weekend_Commits_Pct': round(weekend_pct, 1),
                'Night_Commits_Pct': round(night_pct, 1),
                'Total_Files_Changed': total_files_changed,
                'LOC_Added': total_loc_added,
                'LOC_Deleted': total_loc_deleted,
                'LOC_Modified': total_loc_modified,
                'LOC_Net_Change': total_loc_net,
                'Non_Whitespace_Added': total_non_ws_added,
                'Non_Whitespace_Deleted': total_non_ws_deleted,
                'Non_Whitespace_Modified': total_non_ws_modified,
                'Non_Whitespace_Net': total_non_ws_net,
                'Non_Whitespace_Ratio': round(non_whitespace_ratio, 3),
                'Avg_Cyclomatic_Complexity': round(avg_complexity, 2),
                'High_Complexity_Files': high_complexity_count,
                'Avg_LOC_Per_Commit': round(avg_loc_per_commit, 1),
                'Avg_Files_Per_Commit': round(avg_files_per_commit, 1),
                'Primary_File_Type': primary_file_type,
                'Add_Changes': metrics['change_types'].get('add', 0),
                'Edit_Changes': metrics['change_types'].get('edit', 0),
                'Delete_Changes': metrics['change_types'].get('delete', 0),
                'Productivity_Score': round(productivity_score, 1),
                'Quality_Score': round(quality_score, 1),
                'Consistency_Score': round(consistency_score, 1),
                'Overall_Contribution_Score': round(overall_contribution_score, 1),
                'First_Commit': first_commit.strftime('%Y-%m-%d') if first_commit else 'Unknown',
                'Last_Commit': last_commit.strftime('%Y-%m-%d') if last_commit else 'Unknown'
            })
        
        df_contributions = pd.DataFrame(contribution_data)
        if not df_contributions.empty:
            df_contributions = df_contributions.sort_values('Overall_Contribution_Score', ascending=False)
            
            print(f"[OK] Analyzed comprehensive contribution metrics for {len(df_contributions)} developers")
            
            # Display key metrics
            summary_cols = ['Developer', 'Total_Commits', 'LOC_Net_Change', 'Non_Whitespace_Ratio', 
                           'Business_Hours_Commits_Pct', 'Overall_Contribution_Score']
            print("\nContribution Metrics Summary:")
            print(df_contributions[summary_cols].to_string(index=False))
            
            # Save as contribution metrics
            output_file = f"{self.analyzer.data_dir}/azdo_contribution_metrics.csv"
            df_contributions.to_csv(output_file, index=False)
            print(f"[SAVED] Saved contribution metrics to: {output_file}")
            
            # Mark as cached
            self.analyzer.mark_analysis_cached(analysis_name, output_file)
            
            # Print comprehensive insights
            print(f"\n=== CONTRIBUTION INSIGHTS ===")
            avg_business_hours = df_contributions['Business_Hours_Commits_Pct'].mean()
            avg_weekend = df_contributions['Weekend_Commits_Pct'].mean()
            avg_quality = df_contributions['Quality_Score'].mean()
            
            print(f"[STATS] Team Averages:")
            print(f"  • Business hours commits: {avg_business_hours:.1f}%")
            print(f"  • Weekend commits: {avg_weekend:.1f}%")
            print(f"  • Code quality score: {avg_quality:.1f}/100")
            print(f"  • Average contribution score: {df_contributions['Overall_Contribution_Score'].mean():.1f}/100")
            
            top_contributor = df_contributions.iloc[0]
            print(f"\n[BEST] Top Contributor: {top_contributor['Developer']}")
            print(f"  • {top_contributor['Total_Commits']} commits, {top_contributor['LOC_Net_Change']:,} net LOC")
            print(f"  • Quality score: {top_contributor['Quality_Score']:.1f}, Consistency: {top_contributor['Consistency_Score']:.1f}")
            
            # Identify work patterns
            night_owls = df_contributions[df_contributions['Night_Commits_Pct'] > 20]
            weekend_warriors = df_contributions[df_contributions['Weekend_Commits_Pct'] > 30]
            
            if not night_owls.empty:
                print(f"\n[NIGHT OWLS] Night Owls (>20% night commits): {', '.join(night_owls['Developer'].tolist())}")
            if not weekend_warriors.empty:
                print(f"[DATE] Weekend Warriors (>30% weekend commits): {', '.join(weekend_warriors['Developer'].tolist())}")
        else:
            print("[WARNING]  No contribution data found")
        
        return df_contributions

    def analyze_advanced_developer_contributions(self):
        """Analyze advanced developer contributions with real metrics"""
        print("\n=== ADVANCED DEVELOPER CONTRIBUTIONS ANALYSIS ===")
        
        # Check if analysis is already cached
        analysis_name = "advanced_developer_contributions"
        if self.analyzer.is_analysis_cached(analysis_name):
            cached_df = self.analyzer.get_cached_dataframe(analysis_name)
            if cached_df is not None:
                print("[STATS] Displaying cached advanced contributions results:")
                summary_cols = ['Developer', 'Productivity_Score', 'Quality_Score', 'Knowledge_Sharing_Score', 'Overall_Score']
                print("\nAdvanced Developer Contributions:")
                print(cached_df[summary_cols].to_string(index=False))
                return cached_df
        
        print("[SEARCH] Running fresh advanced contributions analysis...")
        
        # Get repository ID once at the beginning
        try:
            repo_id = self.analyzer.get_repository_id()
            print(f"  [OK] Repository ID obtained for quality analysis: {repo_id}")
        except Exception as e:
            print(f"  [WARNING] Could not get repository ID: {e}")
            repo_id = None
        
        # Build comprehensive developer metrics
        developer_metrics = defaultdict(lambda: {
            'commits': 0,
            'files_changed': set(),
            'loc_added': 0,
            'loc_modified': 0,
            'complexity_samples': [],
            'collaborators': set(),
            'pull_requests': 0,
            'code_reviews': 0,
            'knowledge_areas': set(),
            'file_types': defaultdict(int),
            'commit_messages': []
        })
        
        # Analyze commits for productivity and quality
        for commit_id, changes_data in self.analyzer.detailed_commits.items():
            commit_info = next((c for c in self.analyzer.commits if c.get('commitId') == commit_id), None)
            if not commit_info:
                continue
            
            author_info = self.analyzer.get_author_info(commit_info.get('author', {}))
            author_key = author_info['unique_name']
            
            developer_metrics[author_key]['commits'] += 1
            developer_metrics[author_key]['commit_messages'].append(commit_info.get('comment', ''))
            
            # Analyze file changes
            changes = changes_data.get('changes', [])
            for change in changes:
                item = change.get('item', {})
                path = item.get('path', '')
                change_type = change.get('changeType', '')
                
                if not path or item.get('isFolder', False):
                    continue
                
                filename = os.path.basename(path)
                file_type = self.analyzer.classify_file_type(filename)
                
                if file_type not in ['other', 'docs']:
                    developer_metrics[author_key]['files_changed'].add(path)
                    developer_metrics[author_key]['file_types'][file_type] += 1
                    developer_metrics[author_key]['knowledge_areas'].add(file_type)
                    
                    # Get file content for quality analysis
                    try:
                        # Use the repo_id that was already fetched at the beginning
                        if repo_id:
                            content = self.analyzer.fetch_file_content(repo_id, commit_id, path)
                        else:
                            content = None
                        if content:
                            file_analysis = self.analyzer.analyze_file_contents(content)
                            complexity = self.analyzer.calculate_cyclomatic_complexity(content, filename)
                            
                            if complexity > 1:
                                developer_metrics[author_key]['complexity_samples'].append(complexity)
                            
                            if change_type == 'add':
                                developer_metrics[author_key]['loc_added'] += file_analysis['sloc']
                            elif change_type == 'edit':
                                developer_metrics[author_key]['loc_modified'] += int(file_analysis['sloc'] * 0.3)
                    except:
                        # Fallback estimates
                        if change_type == 'add':
                            developer_metrics[author_key]['loc_added'] += 40
                        elif change_type == 'edit':
                            developer_metrics[author_key]['loc_modified'] += 12
        
        # Analyze pull requests for collaboration
        for pr in self.analyzer.pull_requests:
            creator_info = self.analyzer.get_author_info(pr.get('createdBy', {}))
            creator_key = creator_info['unique_name']
            
            developer_metrics[creator_key]['pull_requests'] += 1
            
            # Track reviewers as collaborators
            reviewers = pr.get('reviewers', [])
            for reviewer in reviewers:
                reviewer_info = self.analyzer.get_author_info(reviewer)
                reviewer_key = reviewer_info['unique_name']
                
                if reviewer_key != creator_key:
                    developer_metrics[creator_key]['collaborators'].add(reviewer_key)
                    developer_metrics[reviewer_key]['code_reviews'] += 1
                    developer_metrics[reviewer_key]['collaborators'].add(creator_key)
        
        # Calculate advanced metrics
        advanced_data = []
        
        for developer, metrics in developer_metrics.items():
            if metrics['commits'] == 0:
                continue
            
            # Productivity Score (0-100)
            commit_score = min(metrics['commits'] / 10 * 25, 25)  # Up to 25 points for commits
            file_score = min(len(metrics['files_changed']) / 20 * 25, 25)  # Up to 25 points for files
            loc_score = min((metrics['loc_added'] + metrics['loc_modified']) / 1000 * 25, 25)  # Up to 25 points for LOC
            pr_score = min(metrics['pull_requests'] / 5 * 25, 25)  # Up to 25 points for PRs
            productivity_score = commit_score + file_score + loc_score + pr_score
            
            # Quality Score (0-100)
            avg_complexity = sum(metrics['complexity_samples']) / len(metrics['complexity_samples']) if metrics['complexity_samples'] else 5
            complexity_score = max(0, (10 - min(avg_complexity, 10)) * 10)  # Lower complexity = higher score
            
            # Analyze commit message quality
            commit_messages = metrics['commit_messages']
            detailed_messages = sum(1 for msg in commit_messages if len(msg.split()) >= 3)
            message_quality = (detailed_messages / max(len(commit_messages), 1)) * 40
            
            # Code review participation
            review_score = min(metrics['code_reviews'] / 10 * 20, 20)
            
            quality_score = complexity_score + message_quality + review_score
            
            # Knowledge Sharing Score (0-100)
            knowledge_breadth = min(len(metrics['knowledge_areas']) / 5 * 30, 30)  # Up to 30 for breadth
            collaboration_score = min(len(metrics['collaborators']) / 5 * 40, 40)  # Up to 40 for collaboration
            pr_sharing = min(metrics['pull_requests'] / 10 * 30, 30)  # Up to 30 for PR creation
            knowledge_sharing_score = knowledge_breadth + collaboration_score + pr_sharing
            
            # Specialization analysis
            primary_language = max(metrics['file_types'].items(), key=lambda x: x[1])[0] if metrics['file_types'] else 'unknown'
            
            advanced_data.append({
                'Developer': developer,
                'Commits': metrics['commits'],
                'Files_Changed': len(metrics['files_changed']),
                'LOC_Added': metrics['loc_added'],
                'LOC_Modified': metrics['loc_modified'],
                'Pull_Requests': metrics['pull_requests'],
                'Code_Reviews': metrics['code_reviews'],
                'Collaborators': len(metrics['collaborators']),
                'Collaborator_List': ', '.join(sorted(list(metrics['collaborators']))),
                'Knowledge_Areas': len(metrics['knowledge_areas']),
                'Knowledge_List': ', '.join(sorted(list(metrics['knowledge_areas']))),
                'Primary_Language': primary_language,
                'Avg_Complexity': round(avg_complexity, 2),
                'Detailed_Messages_Pct': round((detailed_messages / max(len(commit_messages), 1)) * 100, 1),
                'Productivity_Score': round(productivity_score, 1),
                'Quality_Score': round(quality_score, 1),
                'Knowledge_Sharing_Score': round(knowledge_sharing_score, 1),
                'Overall_Score': round((productivity_score + quality_score + knowledge_sharing_score) / 3, 1)
            })
        
        # Create DataFrame and save
        df_advanced = pd.DataFrame(advanced_data)
        if not df_advanced.empty:
            df_advanced = df_advanced.sort_values('Overall_Score', ascending=False)
            
            print(f"[OK] Analyzed advanced contributions for {len(df_advanced)} developers")
            
            # Display summary
            summary_cols = ['Developer', 'Productivity_Score', 'Quality_Score', 'Knowledge_Sharing_Score', 'Overall_Score']
            print("\nAdvanced Developer Contributions:")
            print(df_advanced[summary_cols].to_string(index=False))
            
            # Save results
            output_file = f"{self.analyzer.data_dir}/azdo_advanced_developer_contributions.csv"
            df_advanced.to_csv(output_file, index=False)
            print(f"[SAVED] Saved advanced contributions to: {output_file}")
            
            # Mark as cached
            self.analyzer.mark_analysis_cached(analysis_name, output_file)
            
            # Generate insights
            print(f"\n=== ADVANCED CONTRIBUTION INSIGHTS ===")
            top_overall = df_advanced.iloc[0]
            print(f"[BEST] Top Overall Contributor: {top_overall['Developer']}")
            print(f"  • Overall Score: {top_overall['Overall_Score']}/100")
            print(f"  • Productivity: {top_overall['Productivity_Score']}/100")
            print(f"  • Quality: {top_overall['Quality_Score']}/100")
            print(f"  • Knowledge Sharing: {top_overall['Knowledge_Sharing_Score']}/100")
            
            # Find specialists and generalists
            top_productivity = df_advanced.nlargest(1, 'Productivity_Score').iloc[0]
            top_quality = df_advanced.nlargest(1, 'Quality_Score').iloc[0]
            top_knowledge = df_advanced.nlargest(1, 'Knowledge_Sharing_Score').iloc[0]
            
            print(f"\n[TARGET] Category Leaders:")
            print(f"  • Most Productive: {top_productivity['Developer']} ({top_productivity['Productivity_Score']}/100)")
            print(f"  • Highest Quality: {top_quality['Developer']} ({top_quality['Quality_Score']}/100)")
            print(f"  • Best Knowledge Sharing: {top_knowledge['Developer']} ({top_knowledge['Knowledge_Sharing_Score']}/100)")
            
            # Identify collaboration patterns
            high_collaborators = df_advanced[df_advanced['Collaborators'] >= 3]
            if not high_collaborators.empty:
                print(f"\n[COLLABORATORS] Strong Collaborators (3+ team members):")
                for _, dev in high_collaborators.iterrows():
                    print(f"  • {dev['Developer']}: {dev['Collaborators']} collaborators")
        
        return df_advanced
